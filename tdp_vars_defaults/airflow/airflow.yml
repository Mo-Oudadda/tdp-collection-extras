# Copyright 2022 TOSIT.IO
# SPDX-License-Identifier: Apache-2.0

---
# Airflow Version
airflow_app_name: airflow
airflow_version: 2.5.0

# Airflow users and group
airflow_user: airflow
airflow_group: airflow

# Airflow directories
airflow_root_dir: /opt/tdp
airflow_install_dir: "{{ airflow_root_dir }}/{{ airflow_app_name }}"
airflow_log_dir: /var/log/airflow
airflow_root_conf_dir: /etc/airflow
airflow_pid_dir: /run/airflow
airflow_dag_dir: "{{ airflow_install_dir }}/dags"
airflow_plugins_dir: "{{ airflow_install_dir }}/plugins"
airflow_child_process_log_dir: "{{ airflow_log_dir }}/scheduler"
airflow_certs_dir: "{{ airflow_root_conf_dir }}/certs"
airflow_environment_file_dir: /etc/sysconfig/airflow

# Airflow Executable:
airflow_executable: "{{ airflow_install_dir }}/bin/airflow"

# Airflow python
airflow_python: python3.7
airflow_python_version: 3.7.12
airflow_constraints_file: constraints-3-7.txt

# Python dependencies
airflow_required_python_packages:
  - psycopg2-binary

# Airflow extras and providers packages: # https://airflow.apache.org/docs/apache-airflow/2.2.3/extra-packages-ref.html
airflow_extra_packages:
  - flower
  - redis
  - kerberos
  - celery
  - apache-airflow-providers-apache-hdfs
  - apache-airflow-providers-apache-hive
  - apache-airflow-providers-apache-spark

# Required packages
airflow_required_packages:
  - gcc-c++
  - libffi-devel
  - mariadb-devel
  - python-devel.x86_64
  - python3-devel.x86_64
  - cyrus-sasl-devel.x86_64
  - krb5-devel
  - bzip2-devel 
  - sqlite-devel
  - xz-devel

# Airflow database
airflow_db_user: airflow
airflow_db_pass: airflow
airflow_db_host: master-03
airflow_db_port: 5432
airflow_db_name: airflow
airflow_sql_alchemy_conn: "postgresql+psycopg2://{{ airflow_db_user }}:{{ airflow_db_pass }}@{{ airflow_db_host }}:{{ airflow_db_port }}/{{ airflow_db_name }}"

# airflow conf template. for old versions of airflow use airflow-old.cfg template
airflow_conf_template: airflow.cfg

# Airflow config
airflow_web_server_port: 8280
airflow_executor: CeleryExecutor
airflow_result_backend: "db+postgresql://{{ airflow_db_user }}:{{ airflow_db_pass }}@{{ airflow_db_host }}:{{ airflow_db_port }}/{{ airflow_db_name }}"
airflow_broker_url: "redis://master-01:6379/0"

# SSL configuration
certificates_path: /etc/ssl/certs
certificates_cert_filename: "{{ ansible_fqdn }}"
certificates_ca_filename: "tdp_ca"
certificates_public_key: "{{ certificates_path }}/{{ certificates_cert_filename }}.pem"
certificates_private_key: "{{ certificates_path }}/{{ certificates_cert_filename }}.key"
certificates_ca: "{{ certificates_path }}/{{ certificates_ca_filename }}.crt" 
airflow_public_key: "{{ airflow_certs_dir }}/{{ certificates_cert_filename }}.pem"
airflow_private_key: "{{ airflow_certs_dir }}/{{ certificates_cert_filename }}.key"
ca_certificate_file: "{{ airflow_certs_dir }}/{{ certificates_ca_filename }}.crt"

# Kerberos
krb_create_principals_keytabs: yes
airflow_kerberos:
  ccache: /tmp/airflow_krb5_ccache
  keytab: airflow.service.keytab
  principal: airflow/_HOST@REALM.TDP
  kinit_path: kinit
  reinit_frequency: 3600

# Airflow admin user
airflow_admin:
  - name: Admin
    username: admin
    password: admin123
    role: Admin
    firstname: admin
    lastname: admin
    email: admin@tdp

airflow_users:
  - name: tdp_user
    username: tdp_user
    password: tdp_user123
    role: Op
    firstname: tdp_user
    lastname: tdp_user
    email: tdp_user@tdp

# Airflow connections params
airflow_hive:
  conn_id: hiveserver2_tdp
  conn_type: hive_cli
  conn_schema: default
  host: master-03.tdp
  port: 10001
  use_beeline: "true"
  proxy_user: "tdp_user"
  principal: "hive/master-03.tdp@REALM.TDP;transportMode=http;httpPath=cliservice;ssl=true;sslTrustStore=/etc/ssl/certs/truststore.jks;trustStorePassword=Truststore123!"

airflow_spark:
  conn_id: spark_tdp
  conn_type: spark
  conn_host: yarn

# Airflow connections
airflow_connections_reset: true
airflow_connections:
  - conn_id: "{{ airflow_hive.conn_id }}"
    conn-type: "{{ airflow_hive.conn_type }}"
    conn-host: "{{ airflow_hive.host }}"
    conn-schema: "{{ airflow_hive.conn_schema }}"
    conn-port: "{{ airflow_hive.port }}"
    conn-extra: >-
      '{
        "use_beeline": "airflow_hive.use_beeline",
        "principal": "{{ airflow_hive.principal }}",
        "proxy_user": "{{ airflow_hive.proxy_user }}"
      }'
  - conn_id: "{{ airflow_spark.conn_id }}"
    conn-type: "{{ airflow_spark.conn_type }}"
    conn-host: "{{ airflow_spark.conn_host }}"

# Airflow hadoop env variables
airflow_hadoop_env:
  JAVA_HOME: /usr/lib/jvm/jre-1.8.0-openjdk
  HADOOP_HOME: /opt/tdp/hadoop
  HADOOP_CONF_DIR: /etc/hadoop/conf
  SPARK_HOME: /opt/tdp/spark3
  SPARK_CONF_DIR: /etc/spark3/conf

# Upload dags templates
airflow_upload_dags: true